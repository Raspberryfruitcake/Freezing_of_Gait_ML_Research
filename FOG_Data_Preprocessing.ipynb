{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839370a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import os\n",
    "import joblib\n",
    "import gc\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras import layers\n",
    "from scipy.stats import skew\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from joblib import parallel_backend\n",
    "import dill\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162ba02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prefog(dataset,window_length = 1):\n",
    "  dataset.drop(index = list(dataset[dataset['Action'] == 0].index),inplace=True)\n",
    "  window_length = 64*window_length\n",
    "\n",
    "  fog_index=[]\n",
    "  for i in dataset.index:\n",
    "      if dataset.loc[i,'Action'] == 2:\n",
    "        fog_index.append(i)\n",
    "  fog_index\n",
    "\n",
    "\n",
    "\n",
    "  start_indices=[]\n",
    "  for i in fog_index:\n",
    "    if (dataset.loc[i-1,'Action']!=dataset.loc[i,'Action']):\n",
    "      start_indices.append(i)\n",
    "\n",
    "\n",
    "  prefog=[]\n",
    "  for start in start_indices:\n",
    "    prefog_start = [x for x in range(start-window_length,start)]\n",
    "    prefog.append(prefog_start)\n",
    "\n",
    "  prefog = [item for sublist in prefog for item in sublist]\n",
    "\n",
    "  for i in prefog:\n",
    "       dataset.loc[i,'Action'] = 3\n",
    "  dataset['Action'] = dataset['Action'] - 1\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fef8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b620d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>A_F</th>\n",
       "      <th>A_V</th>\n",
       "      <th>A_L</th>\n",
       "      <th>L_F</th>\n",
       "      <th>L_V</th>\n",
       "      <th>L_L</th>\n",
       "      <th>T_F</th>\n",
       "      <th>T_V</th>\n",
       "      <th>T_L</th>\n",
       "      <th>Action</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750000</td>\n",
       "      <td>-30</td>\n",
       "      <td>990</td>\n",
       "      <td>326</td>\n",
       "      <td>-45</td>\n",
       "      <td>972</td>\n",
       "      <td>181</td>\n",
       "      <td>-38</td>\n",
       "      <td>1000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750015</td>\n",
       "      <td>-30</td>\n",
       "      <td>1000</td>\n",
       "      <td>356</td>\n",
       "      <td>-18</td>\n",
       "      <td>981</td>\n",
       "      <td>212</td>\n",
       "      <td>-48</td>\n",
       "      <td>1028</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750031</td>\n",
       "      <td>-20</td>\n",
       "      <td>990</td>\n",
       "      <td>336</td>\n",
       "      <td>18</td>\n",
       "      <td>981</td>\n",
       "      <td>222</td>\n",
       "      <td>-38</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750046</td>\n",
       "      <td>-20</td>\n",
       "      <td>1000</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>222</td>\n",
       "      <td>-19</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750062</td>\n",
       "      <td>0</td>\n",
       "      <td>990</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>212</td>\n",
       "      <td>-29</td>\n",
       "      <td>1038</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886499</th>\n",
       "      <td>2549937</td>\n",
       "      <td>202</td>\n",
       "      <td>1019</td>\n",
       "      <td>188</td>\n",
       "      <td>-890</td>\n",
       "      <td>111</td>\n",
       "      <td>515</td>\n",
       "      <td>-155</td>\n",
       "      <td>971</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>S09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886500</th>\n",
       "      <td>2549953</td>\n",
       "      <td>202</td>\n",
       "      <td>1009</td>\n",
       "      <td>178</td>\n",
       "      <td>-863</td>\n",
       "      <td>157</td>\n",
       "      <td>434</td>\n",
       "      <td>-155</td>\n",
       "      <td>980</td>\n",
       "      <td>281</td>\n",
       "      <td>0</td>\n",
       "      <td>S09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886501</th>\n",
       "      <td>2549968</td>\n",
       "      <td>212</td>\n",
       "      <td>1000</td>\n",
       "      <td>158</td>\n",
       "      <td>-881</td>\n",
       "      <td>203</td>\n",
       "      <td>373</td>\n",
       "      <td>-155</td>\n",
       "      <td>971</td>\n",
       "      <td>281</td>\n",
       "      <td>0</td>\n",
       "      <td>S09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886502</th>\n",
       "      <td>2549984</td>\n",
       "      <td>202</td>\n",
       "      <td>1019</td>\n",
       "      <td>148</td>\n",
       "      <td>-890</td>\n",
       "      <td>240</td>\n",
       "      <td>393</td>\n",
       "      <td>-155</td>\n",
       "      <td>961</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>S09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886503</th>\n",
       "      <td>2550000</td>\n",
       "      <td>202</td>\n",
       "      <td>1019</td>\n",
       "      <td>178</td>\n",
       "      <td>-927</td>\n",
       "      <td>259</td>\n",
       "      <td>434</td>\n",
       "      <td>-155</td>\n",
       "      <td>971</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "      <td>S09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2886504 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time  A_F   A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
       "0         750000  -30   990  326  -45  972  181  -38  1000   29       0  S01\n",
       "1         750015  -30  1000  356  -18  981  212  -48  1028   29       0  S01\n",
       "2         750031  -20   990  336   18  981  222  -38  1038    9       0  S01\n",
       "3         750046  -20  1000  316   36  990  222  -19  1038    9       0  S01\n",
       "4         750062    0   990  316   36  990  212  -29  1038   29       0  S01\n",
       "...          ...  ...   ...  ...  ...  ...  ...  ...   ...  ...     ...  ...\n",
       "2886499  2549937  202  1019  188 -890  111  515 -155   971  291       0  S09\n",
       "2886500  2549953  202  1009  178 -863  157  434 -155   980  281       0  S09\n",
       "2886501  2549968  212  1000  158 -881  203  373 -155   971  281       0  S09\n",
       "2886502  2549984  202  1019  148 -890  240  393 -155   961  252       0  S09\n",
       "2886503  2550000  202  1019  178 -927  259  434 -155   971  262       0  S09\n",
       "\n",
       "[2886504 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path=r\"C:\\Users\\Admin\\Desktop\\Freezing_Gait_Research\\Data_1\"\n",
    "path=r\"C:\\Users\\Admin\\Desktop\\Labelled_Files\"\n",
    "\n",
    "people = []\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person:\n",
    "        people.append(person)\n",
    "for window_length in range(1,5):\n",
    "    for person in people:\n",
    "        name = person.split('R')[0]\n",
    "        print (name)\n",
    "        file = data_path+\"/\"+person\n",
    "        temp = pd.read_csv(file,delimiter= \" \", header = None)\n",
    "        print (person,' is read',end = '\\t')\n",
    "        if 2 in temp[max(temp.columns)].unique():\n",
    "            print ('Adding {} to dataset'.format(person),end = '\\t')\n",
    "            temp.columns = ['time','A_F','A_V','A_L','L_F','L_V','L_L','T_F','T_V','T_L','Action']\n",
    "            temp = label_prefog(temp,window_length).reset_index(drop=True)\n",
    "            temp['name'] = name\n",
    "            print ('{} is labelled'.format(person))\n",
    "            dataset = pd.concat([dataset,temp],axis = 0)\n",
    "\n",
    "        print ('')\n",
    "    dataset.reset_index(drop =True,inplace=True)\n",
    "    to_path = path + \"/raw_labelled\"\n",
    "    to_name = to_path +\"/win_\"+str(window_length)+\".csv\"\n",
    "\n",
    "    dataset.to_csv(to_name,index = False)\n",
    "\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e6ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe[dataframe.Action == act].index)\n",
    "  groups = []\n",
    "  temp = []\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break\n",
    "    temp.append(indices[i])\n",
    "    if indices[i]+1 != indices[i+1]:\n",
    "      group_count+=1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  fs = 64\n",
    "  window_length = 1\n",
    "  # window_length = window_length*fs\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  for i in groups:\n",
    "    required = math.floor(len(i)/(window_length*fs))\n",
    "\n",
    "    req_index = i[0:(required*fs)]\n",
    "\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fd9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_length in range(1,5):\n",
    "\n",
    "  # path = os.getcwd()+\"/dataset_fog_release/dataset\"\n",
    "  name = path+\"/raw_labelled/win_\"+str(window_length)+\".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  activities = []\n",
    "  for act in range(3):\n",
    "    activities.append(create_window(act,window_length,dataframe))\n",
    "  to_write = pd.concat(activities,axis = 0)\n",
    "  to_path = path + \"/windows\"+\"/windowed_\"+str(window_length)+\".csv\"\n",
    "  to_write.to_csv(to_path,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_F A_V A_L L_F L_V L_L T_F T_V T_L A_F_jerk A_V_jerk A_L_jerk L_F_jerk L_V_jerk L_L_jerk T_F_jerk T_V_jerk T_L_jerk angle_trunk_ankle angle_trunk_upper_leg angle_upper_leg_ankle rel_acc_trunk_ankle_x rel_acc_trunk_ankle_y rel_acc_trunk_ankle_z rel_acc_trunk_upper_leg_x rel_acc_trunk_upper_leg_y rel_acc_trunk_upper_leg_z rel_acc_upper_leg_ankle_x rel_acc_upper_leg_ankle_y rel_acc_upper_leg_ankle_z rel_acc_trunk_ankle_mag rel_acc_trunk_upper_leg_mag rel_acc_upper_leg_ankle_mag ankle_magnitude trunk_magnitude upper_leg_magnitude correlation_trunk_ankle correlation_trunk_upper_leg correlation_upper_leg_ankle cross_corr_A_F_A_V cross_corr_A_F_A_L cross_corr_A_F_L_F cross_corr_A_F_L_V cross_corr_A_F_L_L cross_corr_A_F_T_F cross_corr_A_F_T_V "
     ]
    }
   ],
   "source": [
    "#window_length = 1\n",
    "fs = 64\n",
    "for window_length in range(1,5):\n",
    "  w = window_length*fs\n",
    "  FE_path = path + \"/windows/windowed_\"\n",
    "  name = FE_path + str(window_length) + \".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  df = dataframe.drop(columns=['time','Action','name'])\n",
    "\n",
    "  # Add jerk columns for each axis\n",
    "  for axis in [\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]:\n",
    "    df[axis+\"_jerk\"] = df[axis].diff()\n",
    "    \n",
    "  acc_readings = ['A_F', 'A_V', 'A_L', 'L_F', 'L_V', 'L_L', 'T_F', 'T_V', 'T_L']\n",
    "  df_acc = df[acc_readings]\n",
    "\n",
    "  df['ankle_vector'] = df[['A_F', 'A_V', 'A_L']].values.tolist()\n",
    "  df['trunk_vector'] = df[['T_F', 'T_V', 'T_L']].values.tolist()\n",
    "  df['upper_leg_vector'] = df[['L_F', 'L_V', 'L_L']].values.tolist()\n",
    "\n",
    "  df['ankle_vector'] = df['ankle_vector'].apply(np.array)\n",
    "  df['trunk_vector'] = df['trunk_vector'].apply(np.array)\n",
    "  df['upper_leg_vector'] = df['upper_leg_vector'].apply(np.array)\n",
    "\n",
    "  df['angle_trunk_ankle'] = df.apply(lambda row: np.arccos(np.dot(row['trunk_vector'], row['ankle_vector']) / \n",
    "                                     (np.linalg.norm(row['trunk_vector']) * np.linalg.norm(row['ankle_vector']))), axis=1)\n",
    "\n",
    "  df['angle_trunk_upper_leg'] = df.apply(lambda row: np.arccos(np.dot(row['trunk_vector'], row['upper_leg_vector']) / \n",
    "                                         (np.linalg.norm(row['trunk_vector']) * np.linalg.norm(row['upper_leg_vector']))), axis=1)\n",
    "\n",
    "  df['angle_upper_leg_ankle'] = df.apply(lambda row: np.arccos(np.dot(row['upper_leg_vector'], row['ankle_vector']) / \n",
    "                                         (np.linalg.norm(row['upper_leg_vector']) * np.linalg.norm(row['ankle_vector']))), axis=1)\n",
    "\n",
    "  rel_acc_trunk_ankle= df['trunk_vector'] - df['ankle_vector']\n",
    "  rel_acc_trunk_upper_leg= df['trunk_vector'] - df['upper_leg_vector']\n",
    "  rel_acc_upper_leg_ankle= df['upper_leg_vector'] - df['ankle_vector']\n",
    "\n",
    "  df['rel_acc_trunk_ankle_x'] = rel_acc_trunk_ankle.apply(lambda v: v[0])\n",
    "  df['rel_acc_trunk_ankle_y'] = rel_acc_trunk_ankle.apply(lambda v: v[1])\n",
    "  df['rel_acc_trunk_ankle_z'] = rel_acc_trunk_ankle.apply(lambda v: v[2])\n",
    "  df['rel_acc_trunk_upper_leg_x'] = rel_acc_trunk_upper_leg.apply(lambda v: v[0])\n",
    "  df['rel_acc_trunk_upper_leg_y'] = rel_acc_trunk_upper_leg.apply(lambda v: v[1])\n",
    "  df['rel_acc_trunk_upper_leg_z'] = rel_acc_trunk_upper_leg.apply(lambda v: v[2])\n",
    "  df['rel_acc_upper_leg_ankle_x'] = rel_acc_upper_leg_ankle.apply(lambda v: v[0])\n",
    "  df['rel_acc_upper_leg_ankle_y'] = rel_acc_upper_leg_ankle.apply(lambda v: v[1])\n",
    "  df['rel_acc_upper_leg_ankle_z'] = rel_acc_upper_leg_ankle.apply(lambda v: v[2])\n",
    "\n",
    "  df['rel_acc_trunk_ankle_mag'] = rel_acc_trunk_ankle.apply(np.linalg.norm)\n",
    "  df['rel_acc_trunk_upper_leg_mag'] = rel_acc_trunk_upper_leg.apply(np.linalg.norm)\n",
    "  df['rel_acc_upper_leg_ankle_mag'] = rel_acc_upper_leg_ankle.apply(np.linalg.norm)\n",
    "\n",
    "  df['ankle_magnitude'] = df['ankle_vector'].apply(np.linalg.norm)\n",
    "  df['trunk_magnitude'] = df['trunk_vector'].apply(np.linalg.norm)\n",
    "  df['upper_leg_magnitude'] = df['upper_leg_vector'].apply(np.linalg.norm)\n",
    "\n",
    "  df['correlation_trunk_ankle'] = df['trunk_magnitude'].corr(df['ankle_magnitude'])\n",
    "  df['correlation_trunk_upper_leg'] = df['trunk_magnitude'].corr(df['upper_leg_magnitude'])\n",
    "  df['correlation_upper_leg_ankle'] = df['upper_leg_magnitude'].corr(df['ankle_magnitude'])\n",
    "    \n",
    "  cross_correlation_df = pd.DataFrame()\n",
    "\n",
    "  for i in range(0,len(df_acc),w):\n",
    "      window_data = df_acc.iloc[i:i+w]\n",
    "        \n",
    "      # calculate cross-correlation\n",
    "      for col1 in df_acc.columns:\n",
    "          for col2 in df_acc.columns:\n",
    "              if col1 != col2:  # avoid calculating correlation of a column with itself\n",
    "                  corr = np.correlate(window_data[col1], window_data[col2])[0]  \n",
    "                  cross_correlation_df.loc[i//w, 'cross_corr_'+col1+'_'+col2] = corr\n",
    "                    \n",
    "  df = pd.concat([df, cross_correlation_df], axis=1) \n",
    "    \n",
    "  df=df.drop(columns=['ankle_vector', 'trunk_vector', 'upper_leg_vector'])\n",
    "\n",
    "  stat = pd.DataFrame()\n",
    "\n",
    "  col= list(df.columns)\n",
    "  for s in col:\n",
    "    print (s,end=\" \")\n",
    "    mn =[]\n",
    "    mx =[]\n",
    "    mi =[]\n",
    "    var = []\n",
    "    std = []\n",
    "    mav = []\n",
    "    rms =[]\n",
    "    zcr =[]\n",
    "    jerk_mn =[]  # Mean jerk\n",
    "    p2p = []     # Peak-to-peak amplitude\n",
    "    cf = []      # Crest factor\n",
    "    sf = []      # Shape factor\n",
    "    ifac = []    # Impulse factor\n",
    "    skewness = []\n",
    "    sma = []  # signal magnitude area\n",
    "    wl = []   # waveform length\n",
    "    cov=[]\n",
    "    \n",
    "    for i in range(0,len(df),w):\n",
    "        window_data = df[s].iloc[i:i+w]\n",
    "        mn_  = np.mean(window_data)\n",
    "        mx_  = np.max(window_data)\n",
    "        mi_  = np.min(window_data)\n",
    "        var_  = np.var(window_data)\n",
    "        std_  = np.std(window_data)\n",
    "        mav_  = np.mean(abs(window_data))\n",
    "        rms_  = np.sqrt(np.mean((window_data)**2))\n",
    "        zcr_ = ((window_data > 0).astype(int)).diff().abs().sum() / 2.0\n",
    "        jerk_mn_ = np.mean(window_data) if \"jerk\" in s else None  # Only compute mean jerk for jerk columns\n",
    "        p2p_ = mx_ - mi_\n",
    "        cf_ = mx_ / rms_ if rms_ != 0 else np.nan\n",
    "        sf_ = rms_ / mav_ if mav_ != 0 else np.nan\n",
    "        ifac_ = mx_ / mav_ if mav_ != 0 else np.nan\n",
    "        skewness_ = skew(window_data)\n",
    "        sma_ = np.sum(np.abs(window_data))\n",
    "        wl_ = np.sum(np.abs(np.diff(window_data)))\n",
    "        if i + w <= len(df):  # Ensure that the window doesn't extend beyond the data\n",
    "            cov_ = np.cov(df.iloc[i:i+w].values.T)\n",
    "            cov.append(cov_)\n",
    "        else: \n",
    "            cov.append(None)\n",
    "        \n",
    "        mn.append(mn_)\n",
    "        mx.append(mx_)\n",
    "        mi.append(mi_)\n",
    "        var.append(var_)\n",
    "        std.append(std_)\n",
    "        mav.append(mav_)\n",
    "        rms.append(rms_)\n",
    "        zcr.append(zcr_)\n",
    "        jerk_mn.append(jerk_mn_)  # Append mean jerk\n",
    "        p2p.append(p2p_)         # Append peak-to-peak\n",
    "        cf.append(cf_)           # Append crest factor\n",
    "        sf.append(sf_)           # Append shape factor\n",
    "        ifac.append(ifac_)       # Append impulse factor\n",
    "        skewness.append(skewness_)\n",
    "        sma.append(sma_)\n",
    "        wl.append(wl_)\n",
    "        \n",
    "    stat['skew_'+s] = skewness\n",
    "    stat['sma_'+s] = sma\n",
    "    stat['wl_'+s] = wl        \n",
    "    stat['mean_'+s] = mn\n",
    "    stat['max_'+s] = mx\n",
    "    stat['min_'+s] = mi\n",
    "    stat['var_'+s] = var\n",
    "    stat['std_'+s] = std\n",
    "    stat['rms_'+s] = rms\n",
    "    stat['mav_'+s] = mav\n",
    "    stat['zcr_'+s] = zcr\n",
    "    stat['p2p_'+s] = p2p\n",
    "    stat['cf_'+s] = cf\n",
    "    stat['sf_'+s] = sf\n",
    "    stat['ifac_'+s] = ifac\n",
    "    stat['cov_'+s] = cov\n",
    "    if \"jerk\" in s:\n",
    "        stat['mean_jerk_'+s] = jerk_mn  # Add mean jerk column to stat dataframe\n",
    "\n",
    "  import copy\n",
    "  stat1 = copy.copy(stat)\n",
    "  stat1['w'] = dataframe['Action'].iloc[[x for x in range(0,len(dataframe),w)]].to_list()\n",
    "  order = ['w']\n",
    "  order += stat1.columns.to_list()[:-1]\n",
    "  stat1 = stat1[order]\n",
    "  stat1.columns\n",
    "  col = stat1.columns.to_list()\n",
    "  col[0] = 0\n",
    "  stat1.columns = col\n",
    "  feature_name = path + \"/features/time_\"+str(window_length)+\".csv\"\n",
    "  stat1.to_csv(feature_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter, welch\n",
    "\n",
    "#window_length = 3\n",
    "fs = 64\n",
    "for window_length in range(1,5):\n",
    "  w = window_length*fs\n",
    "  FE_path = path + \"/windows/windowed_\"\n",
    "  name = FE_path + str(window_length) + \".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  df = dataframe.drop(columns=['time','Action','name'])\n",
    "\n",
    "  col= list(df.columns)\n",
    "\n",
    "  order=5\n",
    "\n",
    "  fi=pd.DataFrame()\n",
    "\n",
    "  power = pd.DataFrame()\n",
    "  bands = {'locomotor' :(0.5,3),'freeze' :(3,8)}\n",
    "  \n",
    "  # For dominant frequency and PSD\n",
    "  dom_freq_df = pd.DataFrame()\n",
    "  PSD_df = pd.DataFrame()\n",
    "\n",
    "  for s in col:\n",
    "      xtemp = []\n",
    "      xtemp1 = []\n",
    "      \n",
    "      dom_freq_temp = []  # for dominant frequency\n",
    "      PSD_temp = []  # for PSD\n",
    "      \n",
    "      for i in range(0,len(df),w):\n",
    "          nyq=0.5*fs\n",
    "\n",
    "          #locomotor band 0.5-3hz\n",
    "          loc_low= 0.5/nyq\n",
    "          loc_high=3/nyq\n",
    "\n",
    "          #clipping off band from the window\n",
    "          b, a = butter(order, [loc_low, loc_high], btype='band')\n",
    "          y=lfilter(b,a,df[s].iloc[i:i+w])\n",
    "\n",
    "          #total power in locomotor band\n",
    "          e1=sum([x**2 for x in y])\n",
    "\n",
    "          #freeze band 3-8hz\n",
    "          frez_low= 3/nyq\n",
    "          frez_high=8/nyq\n",
    "\n",
    "          #clipping off band from the window\n",
    "          b1, a1 = butter(order, [frez_low, frez_high], btype='band')\n",
    "          y1=lfilter(b1,a1,df[s].iloc[i:i+w])\n",
    "          #total power in locomotor band\n",
    "          e2=sum([x**2 for x in y1])\n",
    "\n",
    "          FI=e2/e1\n",
    "          POW=e2+e1\n",
    "          xtemp.append(FI)\n",
    "          xtemp1.append(POW)\n",
    "          \n",
    "          # Compute PSD and dominant frequency\n",
    "          freq, psd = welch(df[s].iloc[i:i+w], fs)\n",
    "          dom_freq = freq[np.argmax(psd)]  # dominant frequency\n",
    "          total_psd = np.sum(psd)  # total power spectral density\n",
    "          \n",
    "          dom_freq_temp.append(dom_freq)\n",
    "          PSD_temp.append(total_psd)\n",
    "\n",
    "      fi['FI'+s] = xtemp\n",
    "      power['P'+s] = xtemp1\n",
    "      \n",
    "      dom_freq_df['dom_freq_'+s] = dom_freq_temp  # add to df\n",
    "      PSD_df['PSD_'+s] = PSD_temp  # add to df\n",
    "\n",
    "  print (\"Freeze, power, dominant frequency, and PSD done\")\n",
    "\n",
    "  w = window_length*fs\n",
    "  E=[]\n",
    "  for i in range(0,len(df),w):\n",
    "    energy = np.sum((df.iloc[i:i+w,:])**2)\n",
    "    E.append(energy)\n",
    "  E = pd.DataFrame(E)\n",
    "  E.columns = [\"EN_\" + x for x in df.columns]\n",
    "\n",
    "  #Entropy\n",
    "  from scipy.signal import periodogram\n",
    "\n",
    "  peak_f = pd.DataFrame()\n",
    "  PSE = pd.DataFrame()\n",
    "  for s in col:\n",
    "    peakF = []\n",
    "    pse = []\n",
    "    for i in range(0,len(df),w):\n",
    "        f,Pxx_den = periodogram(df[s].iloc[i:i+w],fs)\n",
    "        p_norm = Pxx_den/sum(Pxx_den)\n",
    "        p_norm = list(filter(lambda a: a != 0, p_norm))\n",
    "        pse.append(-(np.sum(p_norm*np.log(p_norm))))\n",
    "        peak = (fs/w)*max(Pxx_den)\n",
    "        peakF.append(peak)\n",
    "    PSE['ENt_'+s] = pse\n",
    "    peak_f['peak_'+s] = peakF\n",
    "  PSE.fillna(0,inplace = True)\n",
    "\n",
    "  # Concatenating all the dataframes to create a final dataframe with all the features\n",
    "  freq = pd.concat([fi,power,E,PSE,peak_f, dom_freq_df, PSD_df],axis = 1)\n",
    "\n",
    "  feature_name = path + \"/features/freq_\"+str(window_length)+\".csv\"\n",
    "  freq.to_csv(feature_name, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf2819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bright",
   "language": "python",
   "name": "bright"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
