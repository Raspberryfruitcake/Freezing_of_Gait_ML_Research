{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "839370a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import os\n",
    "import joblib\n",
    "import gc\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras import layers\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.integrate import simps\n",
    "from numpy.fft import fft\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.signal import butter, lfilter, welch\n",
    "from joblib import parallel_backend\n",
    "import dill\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "162ba02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prefog(dataset,window_length = 1):\n",
    "  dataset.drop(index = list(dataset[dataset['Action'] == 0].index),inplace=True)\n",
    "  window_length = 64*window_length\n",
    "\n",
    "  fog_index=[]\n",
    "  for i in dataset.index:\n",
    "      if dataset.loc[i,'Action'] == 2:\n",
    "        fog_index.append(i)\n",
    "  fog_index\n",
    "\n",
    "  start_indices=[]\n",
    "  for i in fog_index:\n",
    "    if (dataset.loc[i-1,'Action']!=dataset.loc[i,'Action']):\n",
    "      start_indices.append(i)\n",
    "\n",
    "\n",
    "  prefog=[]\n",
    "  for start in start_indices:\n",
    "    prefog_start = [x for x in range(start-window_length,start)]\n",
    "    prefog.append(prefog_start)\n",
    "\n",
    "  prefog = [item for sublist in prefog for item in sublist]\n",
    "\n",
    "  for i in prefog:\n",
    "       dataset.loc[i,'Action'] = 3\n",
    "  dataset['Action'] = dataset['Action'] - 1\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fef8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b620d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>A_F</th>\n",
       "      <th>A_V</th>\n",
       "      <th>A_L</th>\n",
       "      <th>L_F</th>\n",
       "      <th>L_V</th>\n",
       "      <th>L_L</th>\n",
       "      <th>T_F</th>\n",
       "      <th>T_V</th>\n",
       "      <th>T_L</th>\n",
       "      <th>Action</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750000</td>\n",
       "      <td>-30</td>\n",
       "      <td>990</td>\n",
       "      <td>326</td>\n",
       "      <td>-45</td>\n",
       "      <td>972</td>\n",
       "      <td>181</td>\n",
       "      <td>-38</td>\n",
       "      <td>1000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750015</td>\n",
       "      <td>-30</td>\n",
       "      <td>1000</td>\n",
       "      <td>356</td>\n",
       "      <td>-18</td>\n",
       "      <td>981</td>\n",
       "      <td>212</td>\n",
       "      <td>-48</td>\n",
       "      <td>1028</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750031</td>\n",
       "      <td>-20</td>\n",
       "      <td>990</td>\n",
       "      <td>336</td>\n",
       "      <td>18</td>\n",
       "      <td>981</td>\n",
       "      <td>222</td>\n",
       "      <td>-38</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750046</td>\n",
       "      <td>-20</td>\n",
       "      <td>1000</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>222</td>\n",
       "      <td>-19</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750062</td>\n",
       "      <td>0</td>\n",
       "      <td>990</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>212</td>\n",
       "      <td>-29</td>\n",
       "      <td>1038</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848655</th>\n",
       "      <td>1199937</td>\n",
       "      <td>-242</td>\n",
       "      <td>852</td>\n",
       "      <td>534</td>\n",
       "      <td>781</td>\n",
       "      <td>277</td>\n",
       "      <td>464</td>\n",
       "      <td>-58</td>\n",
       "      <td>952</td>\n",
       "      <td>378</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848656</th>\n",
       "      <td>1199953</td>\n",
       "      <td>-242</td>\n",
       "      <td>852</td>\n",
       "      <td>564</td>\n",
       "      <td>781</td>\n",
       "      <td>268</td>\n",
       "      <td>444</td>\n",
       "      <td>-48</td>\n",
       "      <td>952</td>\n",
       "      <td>388</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848657</th>\n",
       "      <td>1199968</td>\n",
       "      <td>-262</td>\n",
       "      <td>862</td>\n",
       "      <td>554</td>\n",
       "      <td>781</td>\n",
       "      <td>268</td>\n",
       "      <td>454</td>\n",
       "      <td>-67</td>\n",
       "      <td>952</td>\n",
       "      <td>368</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848658</th>\n",
       "      <td>1199984</td>\n",
       "      <td>-252</td>\n",
       "      <td>882</td>\n",
       "      <td>534</td>\n",
       "      <td>781</td>\n",
       "      <td>268</td>\n",
       "      <td>464</td>\n",
       "      <td>-48</td>\n",
       "      <td>952</td>\n",
       "      <td>407</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848659</th>\n",
       "      <td>1200000</td>\n",
       "      <td>-252</td>\n",
       "      <td>862</td>\n",
       "      <td>554</td>\n",
       "      <td>763</td>\n",
       "      <td>277</td>\n",
       "      <td>474</td>\n",
       "      <td>-48</td>\n",
       "      <td>942</td>\n",
       "      <td>388</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>848660 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time  A_F   A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
       "0        750000  -30   990  326  -45  972  181  -38  1000   29       0  S01\n",
       "1        750015  -30  1000  356  -18  981  212  -48  1028   29       0  S01\n",
       "2        750031  -20   990  336   18  981  222  -38  1038    9       0  S01\n",
       "3        750046  -20  1000  316   36  990  222  -19  1038    9       0  S01\n",
       "4        750062    0   990  316   36  990  212  -29  1038   29       0  S01\n",
       "...         ...  ...   ...  ...  ...  ...  ...  ...   ...  ...     ...  ...\n",
       "848655  1199937 -242   852  534  781  277  464  -58   952  378       0  S02\n",
       "848656  1199953 -242   852  564  781  268  444  -48   952  388       0  S02\n",
       "848657  1199968 -262   862  554  781  268  454  -67   952  368       0  S02\n",
       "848658  1199984 -252   882  534  781  268  464  -48   952  407       0  S02\n",
       "848659  1200000 -252   862  554  763  277  474  -48   942  388       0  S02\n",
       "\n",
       "[848660 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path=r\"C:\\Users\\Admin\\Desktop\\Freezing_Gait_Research\\Data_Test\"\n",
    "path=r\"C:\\Users\\Admin\\Desktop\\Labelled_Files_1\"\n",
    "\n",
    "people = []\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person:\n",
    "        people.append(person)\n",
    "for window_length in range(1,5):\n",
    "    for person in people:\n",
    "        name = person.split('R')[0]\n",
    "        print (name)\n",
    "        file = data_path+\"/\"+person\n",
    "        temp = pd.read_csv(file,delimiter= \" \", header = None)\n",
    "        print (person,' is read',end = '\\t')\n",
    "        if 2 in temp[max(temp.columns)].unique():\n",
    "            print ('Adding {} to dataset'.format(person),end = '\\t')\n",
    "            temp.columns = ['time','A_F','A_V','A_L','L_F','L_V','L_L','T_F','T_V','T_L','Action']\n",
    "            temp = label_prefog(temp,window_length).reset_index(drop=True)\n",
    "            temp['name'] = name\n",
    "            print ('{} is labelled'.format(person))\n",
    "            dataset = pd.concat([dataset,temp],axis = 0)\n",
    "\n",
    "        print ('')\n",
    "    dataset.reset_index(drop =True,inplace=True)\n",
    "    to_path = path + \"/raw_labelled\"\n",
    "    to_name = to_path +\"/win_\"+str(window_length)+\".csv\"\n",
    "\n",
    "    dataset.to_csv(to_name,index = False)\n",
    "\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "16e6ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe[dataframe.Action == act].index)\n",
    "  groups = []\n",
    "  temp = []\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break\n",
    "    temp.append(indices[i])\n",
    "    if indices[i]+1 != indices[i+1]:\n",
    "      group_count+=1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  fs = 64\n",
    "  window_length = 1\n",
    "  # window_length = window_length*fs\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  for i in groups:\n",
    "    required = math.floor(len(i)/(window_length*fs))\n",
    "\n",
    "    req_index = i[0:(required*fs)]\n",
    "\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85fd9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_length in range(1,5):\n",
    "\n",
    "  # path = os.getcwd()+\"/dataset_fog_release/dataset\"\n",
    "  name = path+\"/raw_labelled/win_\"+str(window_length)+\".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  activities = []\n",
    "  for act in range(3):\n",
    "    activities.append(create_window(act,window_length,dataframe))\n",
    "  to_write = pd.concat(activities,axis = 0)\n",
    "  to_path = path + \"/windows\"+\"/windowed_\"+str(window_length)+\".csv\"\n",
    "  to_write.to_csv(to_path,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "65d8e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_F A_V A_L L_F L_V L_L T_F T_V T_L A_F A_V A_L L_F L_V L_L T_F T_V T_L A_F A_V A_L L_F L_V L_L T_F T_V T_L A_F A_V A_L L_F L_V L_L T_F T_V T_L "
     ]
    }
   ],
   "source": [
    "fs = 64\n",
    "for window_length in range(1,5):\n",
    "  w = window_length*fs\n",
    "  FE_path = path + \"/windows/windowed_\"\n",
    "  name = FE_path + str(window_length) + \".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  df = dataframe.drop(columns=['time','Action','name'])\n",
    "    \n",
    "  acc_readings = ['A_F', 'A_V', 'A_L', 'L_F', 'L_V', 'L_L', 'T_F', 'T_V', 'T_L']\n",
    "  df_acc = df[acc_readings]\n",
    "\n",
    "  stat = pd.DataFrame()\n",
    "\n",
    "  col= list(df.columns)\n",
    "  for s in col:\n",
    "    print (s,end=\" \")\n",
    "    mn =[]\n",
    "    mx =[]\n",
    "    mi =[]\n",
    "    var = []\n",
    "    std = []\n",
    "    mav = []\n",
    "    rms =[]\n",
    "    zcr =[]\n",
    "    skewness = []\n",
    "    sma = []\n",
    "    wl = []\n",
    "    p2p = []     # Peak-to-peak amplitude\n",
    "    cf = []      # Crest factor\n",
    "    sf = []      # Shape factor\n",
    "    ifac = []    # Impulse factor\n",
    "    for i in range(0,len(df),w):\n",
    "        window_data = df[s].iloc[i:i+w]\n",
    "        mn_  = np.mean(window_data)\n",
    "        mx_  = np.max(window_data)\n",
    "        mi_  = np.min(window_data)\n",
    "        var_  = np.var(window_data)\n",
    "        std_  = np.std(window_data)\n",
    "        mav_  = np.mean(abs(window_data))\n",
    "        rms_  = np.sqrt(np.mean((window_data)**2))\n",
    "        zcr_ = ((window_data > 0).astype(int)).diff().abs().sum() / 2.0\n",
    "        p2p_ = mx_ - mi_\n",
    "        cf_ = mx_ / rms_ if rms_ != 0 else np.nan\n",
    "        sf_ = rms_ / mav_ if mav_ != 0 else np.nan\n",
    "        ifac_ = mx_ / mav_ if mav_ != 0 else np.nan\n",
    "        skewness_ = skew(window_data)\n",
    "        sma_ = np.sum(np.abs(window_data))\n",
    "        wl_ = np.sum(np.abs(np.diff(window_data)))\n",
    "        \n",
    "        mn.append(mn_)\n",
    "        mx.append(mx_)\n",
    "        mi.append(mi_)\n",
    "        var.append(var_)\n",
    "        std.append(std_)\n",
    "        mav.append(mav_)\n",
    "        rms.append(rms_)\n",
    "        zcr.append(zcr_)\n",
    "        p2p.append(p2p_)         # Append peak-to-peak\n",
    "        cf.append(cf_)           # Append crest factor\n",
    "        sf.append(sf_)           # Append shape factor\n",
    "        ifac.append(ifac_)       # Append impulse factor\n",
    "        skewness.append(skewness_)\n",
    "        sma.append(sma_)\n",
    "        wl.append(wl_)\n",
    "        \n",
    "    stat['mean_'+s] = mn\n",
    "    stat['max_'+s] = mx\n",
    "    stat['min_'+s] = mi\n",
    "    stat['var_'+s] = var\n",
    "    stat['std_'+s] = std\n",
    "    stat['rms_'+s] = rms\n",
    "    stat['mav_'+s] = mav\n",
    "    stat['zcr_'+s] = zcr\n",
    "    stat['p2p_'+s] = p2p\n",
    "    stat['cf_'+s] = cf\n",
    "    stat['sf_'+s] = sf\n",
    "    stat['ifac_'+s] = ifac\n",
    "    stat['skew_'+s] = skewness\n",
    "    stat['sma_'+s] = sma\n",
    "    stat['wl_'+s] = wl\n",
    "\n",
    "  # Create empty dataframes for new features\n",
    "  cross_correlation_df = pd.DataFrame()\n",
    "  covariance_df = pd.DataFrame()\n",
    "\n",
    "  # Calculate cross-correlation and covariance for each window\n",
    "  for i in range(0,len(df_acc),w):\n",
    "      window_data = df_acc.iloc[i:i+w]\n",
    "        \n",
    "      # calculate cross-correlation\n",
    "      for col1 in df_acc.columns:\n",
    "          for col2 in df_acc.columns:\n",
    "              if col1 != col2:  # avoid calculating correlation of a column with itself\n",
    "                  corr = np.correlate(window_data[col1], window_data[col2])[0]  \n",
    "                  cross_correlation_df.loc[i//w, 'corr_'+col1+'_'+col2] = corr\n",
    "                    \n",
    "      # calculate covariance\n",
    "      cov_matrix = np.cov(window_data, rowvar=False)\n",
    "      cov_df = pd.DataFrame(cov_matrix, index=acc_readings, columns=acc_readings)\n",
    "        \n",
    "      # flatten the covariance matrix and add to dataframe\n",
    "      for col1 in cov_df.columns:\n",
    "          for col2 in cov_df.columns:\n",
    "              if col1 != col2:  # avoid calculating covariance of a column with itself\n",
    "                  covariance_df.loc[i//w, 'cov_'+col1+'_'+col2] = cov_df.loc[col1, col2]\n",
    "\n",
    "  # Concatenate the cross-correlation and covariance features with the existing dataframe\n",
    "  stat = pd.concat([stat, cross_correlation_df, covariance_df], axis=1)\n",
    "    \n",
    "  # Add jerk columns for each axis\n",
    "  for axis in [\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]:\n",
    "    stat[axis+\"_jerk\"] = df[axis].diff()\n",
    "\n",
    "  df['ankle_vector'] = df[['A_F', 'A_V', 'A_L']].values.tolist()\n",
    "  df['trunk_vector'] = df[['T_F', 'T_V', 'T_L']].values.tolist()\n",
    "  df['upper_leg_vector'] = df[['L_F', 'L_V', 'L_L']].values.tolist()\n",
    "\n",
    "  df['ankle_vector'] = df['ankle_vector'].apply(np.array)\n",
    "  df['trunk_vector'] = df['trunk_vector'].apply(np.array)\n",
    "  df['upper_leg_vector'] = df['upper_leg_vector'].apply(np.array)\n",
    "    \n",
    "  stat['angle_trunk_ankle'] = df.apply(lambda row: np.arccos(np.dot(row['trunk_vector'], row['ankle_vector']) / \n",
    "                                     (np.linalg.norm(row['trunk_vector']) * np.linalg.norm(row['ankle_vector']))), axis=1)\n",
    "\n",
    "  stat['angle_trunk_upper_leg'] = df.apply(lambda row: np.arccos(np.dot(row['trunk_vector'], row['upper_leg_vector']) / \n",
    "                                         (np.linalg.norm(row['trunk_vector']) * np.linalg.norm(row['upper_leg_vector']))), axis=1)\n",
    "\n",
    "  stat['angle_upper_leg_ankle'] = df.apply(lambda row: np.arccos(np.dot(row['upper_leg_vector'], row['ankle_vector']) / \n",
    "                                         (np.linalg.norm(row['upper_leg_vector']) * np.linalg.norm(row['ankle_vector']))), axis=1)\n",
    "    \n",
    "  rel_acc_trunk_ankle= df['trunk_vector'] - df['ankle_vector']\n",
    "  rel_acc_trunk_upper_leg= df['trunk_vector'] - df['upper_leg_vector']\n",
    "  rel_acc_upper_leg_ankle= df['upper_leg_vector'] - df['ankle_vector']\n",
    "    \n",
    "  stat['rel_acc_trunk_ankle_mag'] = rel_acc_trunk_ankle.apply(np.linalg.norm)\n",
    "  stat['rel_acc_trunk_upper_leg_mag'] = rel_acc_trunk_upper_leg.apply(np.linalg.norm)\n",
    "  stat['rel_acc_upper_leg_ankle_mag'] = rel_acc_upper_leg_ankle.apply(np.linalg.norm)\n",
    "\n",
    "  stat['rel_acc_trunk_ankle_x'] = rel_acc_trunk_ankle.apply(lambda v: v[0])\n",
    "  stat['rel_acc_trunk_ankle_y'] = rel_acc_trunk_ankle.apply(lambda v: v[1])\n",
    "  stat['rel_acc_trunk_ankle_z'] = rel_acc_trunk_ankle.apply(lambda v: v[2])\n",
    "  stat['rel_acc_trunk_upper_leg_x'] = rel_acc_trunk_upper_leg.apply(lambda v: v[0])\n",
    "  stat['rel_acc_trunk_upper_leg_y'] = rel_acc_trunk_upper_leg.apply(lambda v: v[1])\n",
    "  stat['rel_acc_trunk_upper_leg_z'] = rel_acc_trunk_upper_leg.apply(lambda v: v[2])\n",
    "  stat['rel_acc_upper_leg_ankle_x'] = rel_acc_upper_leg_ankle.apply(lambda v: v[0])\n",
    "  stat['rel_acc_upper_leg_ankle_y'] = rel_acc_upper_leg_ankle.apply(lambda v: v[1])\n",
    "  stat['rel_acc_upper_leg_ankle_z'] = rel_acc_upper_leg_ankle.apply(lambda v: v[2])\n",
    "\n",
    "  stat['ankle_magnitude'] = df['ankle_vector'].apply(np.linalg.norm)\n",
    "  stat['trunk_magnitude'] = df['trunk_vector'].apply(np.linalg.norm)\n",
    "  stat['upper_leg_magnitude'] = df['upper_leg_vector'].apply(np.linalg.norm)\n",
    "\n",
    "  df=df.drop(columns=['ankle_vector', 'trunk_vector', 'upper_leg_vector'])\n",
    "\n",
    "  import copy\n",
    "  stat1 = copy.copy(stat)\n",
    "  stat1['w'] = dataframe['Action'].iloc[[x for x in range(0,len(dataframe),w)]].to_list()\n",
    "  order = ['w']\n",
    "  order += stat1.columns.to_list()[:-1]\n",
    "  stat1 = stat1[order]\n",
    "  stat1.columns\n",
    "  col = stat1.columns.to_list()\n",
    "  col[0] = 0\n",
    "  stat1.columns = col\n",
    "  feature_name = path + \"/features/time_\"+str(window_length)+\".csv\"\n",
    "  stat1.to_csv(feature_name, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8b72fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze, power, dominant frequency, PSD and other features done\n",
      "Freeze, power, dominant frequency, PSD and other features done\n",
      "Freeze, power, dominant frequency, PSD and other features done\n",
      "Freeze, power, dominant frequency, PSD and other features done\n"
     ]
    }
   ],
   "source": [
    "#window_length = 3\n",
    "fs = 64\n",
    "for window_length in range(1,5):\n",
    "  w = window_length*fs\n",
    "  FE_path = path + \"/windows/windowed_\"\n",
    "  name = FE_path + str(window_length) + \".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  df = dataframe.drop(columns=['time','Action','name'])\n",
    "\n",
    "  col= list(df.columns)\n",
    "\n",
    "  order=5\n",
    "\n",
    "  fi=pd.DataFrame()\n",
    "\n",
    "  power = pd.DataFrame()\n",
    "  bands = {'locomotor' :(0.5,3),'freeze' :(3,8)}\n",
    "  \n",
    "  # For dominant frequency and PSD\n",
    "  dom_freq_df = pd.DataFrame()\n",
    "  PSD_df = pd.DataFrame()\n",
    "\n",
    "  for s in col:\n",
    "      xtemp = []\n",
    "      xtemp1 = []\n",
    "      \n",
    "      dom_freq_temp = []  # for dominant frequency\n",
    "      PSD_temp = []  # for PSD\n",
    "\n",
    "      spectral_centroid_temp = []  # for spectral centroid\n",
    "      spectral_spread_temp = []  # for spectral spread\n",
    "      spectral_skewness_temp = []  # for spectral skewness\n",
    "      spectral_kurtosis_temp = []  # for spectral kurtosis\n",
    "      spectral_slope_temp = []  # for spectral slope\n",
    "      spectral_flux_temp = []  # for spectral flux\n",
    "      bandpower_temp = []  # for band power\n",
    "\n",
    "      \n",
    "      for i in range(0,len(df),w):\n",
    "          nyq=0.5*fs\n",
    "\n",
    "          #locomotor band 0.5-3hz\n",
    "          loc_low= 0.5/nyq\n",
    "          loc_high=3/nyq\n",
    "\n",
    "          #clipping off band from the window\n",
    "          b, a = butter(order, [loc_low, loc_high], btype='band')\n",
    "          y=lfilter(b,a,df[s].iloc[i:i+w])\n",
    "\n",
    "          #total power in locomotor band\n",
    "          e1=sum([x**2 for x in y])\n",
    "\n",
    "          #freeze band 3-8hz\n",
    "          frez_low= 3/nyq\n",
    "          frez_high=8/nyq\n",
    "\n",
    "          #clipping off band from the window\n",
    "          b1, a1 = butter(order, [frez_low, frez_high], btype='band')\n",
    "          y1=lfilter(b1,a1,df[s].iloc[i:i+w])\n",
    "          #total power in locomotor band\n",
    "          e2=sum([x**2 for x in y1])\n",
    "\n",
    "          FI=e2/e1\n",
    "          POW=e2+e1\n",
    "          xtemp.append(FI)\n",
    "          xtemp1.append(POW)\n",
    "          \n",
    "          # Compute PSD and dominant frequency\n",
    "          freq, psd = welch(df[s].iloc[i:i+w], fs)\n",
    "          dom_freq = freq[np.argmax(psd)]  # dominant frequency\n",
    "          total_psd = np.sum(psd)  # total power spectral density\n",
    "          \n",
    "          dom_freq_temp.append(dom_freq)\n",
    "          PSD_temp.append(total_psd)\n",
    "            \n",
    "          # Compute Spectral Centroid\n",
    "          spectral_centroid = np.sum(freq * psd) / np.sum(psd)\n",
    "          spectral_centroid_temp.append(spectral_centroid)\n",
    "\n",
    "          # Compute Spectral Spread\n",
    "          spectral_spread = np.sqrt(np.sum((freq - spectral_centroid)**2 * psd) / np.sum(psd))\n",
    "          spectral_spread_temp.append(spectral_spread)\n",
    "\n",
    "          # Compute Spectral Skewness\n",
    "          spectral_skewness = np.sum(((freq - spectral_centroid) / spectral_spread)**3 * psd) / np.sum(psd)\n",
    "          spectral_skewness_temp.append(spectral_skewness)\n",
    "\n",
    "          # Compute Spectral Kurtosis\n",
    "          spectral_kurtosis = np.sum(((freq - spectral_centroid) / spectral_spread)**4 * psd) / np.sum(psd) - 3\n",
    "          spectral_kurtosis_temp.append(spectral_kurtosis)\n",
    "\n",
    "          # Compute Spectral Slope\n",
    "          spectral_slope = np.polyfit(freq, psd, 1)[0]\n",
    "          spectral_slope_temp.append(spectral_slope)\n",
    "\n",
    "          # Compute Bandpower\n",
    "          bandpower = simps(psd, freq)\n",
    "          bandpower_temp.append(bandpower)\n",
    "\n",
    "      fi['FI'+s] = xtemp\n",
    "      power['P'+s] = xtemp1\n",
    "      \n",
    "      dom_freq_df['dom_freq_'+s] = dom_freq_temp  # add to df\n",
    "      PSD_df['PSD_'+s] = PSD_temp  # add to df\n",
    "\n",
    "      dom_freq_df['spectral_centroid_'+s] = spectral_centroid_temp  # add to df\n",
    "      PSD_df['spectral_spread_'+s] = spectral_spread_temp  # add to df\n",
    "      dom_freq_df['spectral_skewness_'+s] = spectral_skewness_temp  # add to df\n",
    "      PSD_df['spectral_kurtosis_'+s] = spectral_kurtosis_temp  # add to df\n",
    "      dom_freq_df['spectral_slope_'+s] = spectral_slope_temp  # add to df\n",
    "      PSD_df['bandpower_'+s] = bandpower_temp  # add to df\n",
    "\n",
    "  print (\"Freeze, power, dominant frequency, PSD and other features done\")\n",
    "\n",
    "  w = window_length*fs\n",
    "  E=[]\n",
    "  for i in range(0,len(df),w):\n",
    "    energy = np.sum((df.iloc[i:i+w,:])**2)\n",
    "    E.append(energy)\n",
    "  E = pd.DataFrame(E)\n",
    "  E.columns = [\"EN_\" + x for x in df.columns]\n",
    "\n",
    "  #Entropy\n",
    "  from scipy.signal import periodogram\n",
    "\n",
    "  peak_f = pd.DataFrame()\n",
    "  PSE = pd.DataFrame()\n",
    "  for s in col:\n",
    "    peakF = []\n",
    "    pse = []\n",
    "    for i in range(0,len(df),w):\n",
    "        f,Pxx_den = periodogram(df[s].iloc[i:i+w],fs)\n",
    "        p_norm = Pxx_den/sum(Pxx_den)\n",
    "        p_norm = list(filter(lambda a: a != 0, p_norm))\n",
    "        pse.append(-(np.sum(p_norm*np.log(p_norm))))\n",
    "        peak = (fs/w)*max(Pxx_den)\n",
    "        peakF.append(peak)\n",
    "    PSE['ENt_'+s] = pse\n",
    "    peak_f['peak_'+s] = peakF\n",
    "  PSE.fillna(0,inplace = True)\n",
    "\n",
    "  # Concatenating all the dataframes to create a final dataframe with all the features\n",
    "  freq = pd.concat([fi,power,E,PSE,peak_f, dom_freq_df, PSD_df],axis = 1)\n",
    "\n",
    "  feature_name = path + \"/features/freq_\"+str(window_length)+\".csv\"\n",
    "  freq.to_csv(feature_name, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c578a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bright",
   "language": "python",
   "name": "bright"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
